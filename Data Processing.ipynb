{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nd2reader\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import imageio\n",
    "\n",
    "from skimage import data\n",
    "from skimage.feature import register_translation\n",
    "from skimage.feature.register_translation import _upsampled_dft\n",
    "from scipy.ndimage import fourier_shift\n",
    "\n",
    "from skimage.util import *\n",
    "from skimage import exposure\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(reader, data_path):\n",
    "    data = []\n",
    "    filenames = []\n",
    "    for filename in sorted(listdir(data_path)):\n",
    "        if filename.endswith('.nd2'):\n",
    "            data.append(reader(data_path + filename))\n",
    "            filenames.append(filename)\n",
    "    return data, filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_volume(sample, channel, frame):\n",
    "    if channel in range(sample.sizes[\"c\"]) and frame in range(sample.sizes[\"t\"]):\n",
    "        sample.iter_axes = 'z'\n",
    "        sample.default_coords['c'] = channel\n",
    "        sample.default_coords['t'] = frame\n",
    "        \n",
    "        volume = np.array([np.array(level) for level in sample])\n",
    "        return volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_images(img_1, img_2):\n",
    "    shift, error, diffphase = register_translation(img_1, img_2, 100)\n",
    "        \n",
    "    shift = -1 * shift\n",
    "    img_1_shifted = fourier_shift(np.fft.fftn(img_1), shift)\n",
    "    img_1_shifted = np.fft.ifftn(img_1_shifted).real\n",
    "    \n",
    "    return img_1_shifted, img_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_volumes(volume_1, volume_2):\n",
    "    volume_1_shifted = []\n",
    "    for img_1, img_2 in zip(volume_1, volume_2):\n",
    "        img_1_shifted, img_2 = align_images(img_1, img_2)\n",
    "        volume_1_shifted.append(img_1_shifted)\n",
    "    return volume_1_shifted, volume_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(img):\n",
    "    return [img, np.fliplr(img), np.flipud(img), np.fliplr(np.flipud(img))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_volume(volume):\n",
    "    volume_augmented = []\n",
    "    \n",
    "    for img in volume:\n",
    "        volume_augmented.extend(augment_image(img_1))\n",
    "    return volume_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def augment_volumes(volume_1, volume_2):\n",
    "#     volume_1_augmented = []\n",
    "#     volume_2_augmented = []\n",
    "    \n",
    "#     for img_1, img_2 in zip(volume_1, volume_2):\n",
    "#         volume_1_augmented.extend(augment_image(img_1))\n",
    "#         volume_2_augmented.extend(augment_image(img_2))\n",
    "#     return volume_1_augmented, volume_2_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_overnoised(img,bright_pixel_percent=0.05):\n",
    "    hist = exposure.histogram(img, nbins=2)[0]\n",
    "    value = hist[1] # Number of bright points. (The less value, the less noise)\n",
    "    n_pixels = img.shape[0] * img.shape[1]\n",
    "    return value > n_pixels * bright_pixel_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_volumes(volume_1, volume_2):\n",
    "    volume_1_cleaned = []\n",
    "    volume_2_cleaned = []\n",
    "    \n",
    "    for img_1, img_2 in zip(volume_1, volume_2):\n",
    "        if is_overnoised(img_1) | is_overnoised(img_2):\n",
    "            continue\n",
    "        else:\n",
    "            volume_1_cleaned.append(img_1)\n",
    "            volume_2_cleaned.append(img_2)\n",
    "    return volume_1_cleaned, volume_2_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adap_hist_img(img):\n",
    "    image_rescaled = exposure.rescale_intensity(img, in_range=(0, 2**12 - 1))\n",
    "    rescaled_image_histeq = exposure.equalize_adapthist(image_rescaled)\n",
    "    return rescaled_image_histeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adap_hist_volume(volume):\n",
    "    volume_adap_hist = []\n",
    "    \n",
    "    for img in volume:\n",
    "        volume_adap_hist.append(adap_hist_img(img))\n",
    "    return volume_adap_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_img_grayscale(img, filename):\n",
    "    plt.imsave(filename, img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_volume(volume, path):\n",
    "    for ind, image in enumerate(volume):\n",
    "        filename = f\"{path}_image_{ind}.jpg\"\n",
    "        save_img_grayscale(image, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [{'clean':False, 'align':True, 'augment':False},\n",
    "          {'clean':True, 'align':True, 'augment':False},\n",
    "          {'clean':False, 'align':True, 'augment':True},\n",
    "          {'clean':True, 'align':True, 'augment':True}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folders(dataset_path):\n",
    "    dataset_folders = [dataset_path,\n",
    "                       f\"{dataset_path}/green\",\n",
    "                       f\"{dataset_path}/green/train\",\n",
    "                       f\"{dataset_path}/green/train/src\",\n",
    "                       f\"{dataset_path}/green/train/trg\",\n",
    "                       f\"{dataset_path}/green/val\",\n",
    "                       f\"{dataset_path}/green/val/src\",\n",
    "                       f\"{dataset_path}/green/val/trg\",\n",
    "    \n",
    "                       f\"{dataset_path}/red\",\n",
    "                       f\"{dataset_path}/red/train\",\n",
    "                       f\"{dataset_path}/red/train/src\",\n",
    "                       f\"{dataset_path}/red/train/trg\",\n",
    "                       f\"{dataset_path}/red/val\",\n",
    "                       f\"{dataset_path}/red/val/src\",\n",
    "                       f\"{dataset_path}/red/val/trg\"]\n",
    "\n",
    "    for folder in dataset_folders:\n",
    "        os.mkdir(folder)\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(samples, param, dataset_path):\n",
    "    \n",
    "    create_folders(dataset_path)\n",
    "    \n",
    "    for sample_ind, sample in enumerate(samples):\n",
    "        print(\"Sample №{}\".format(sample_ind))\n",
    "            \n",
    "        for channel_ind, channel_name in enumerate([\"green\", \"red\"]):\n",
    "            for pair_ind, pair in enumerate([[0, 2], [1, 3]]): # Frame pairs\n",
    "                \n",
    "                is_test = (pair_ind == 1 and sample_ind == len(samples) - 1)\n",
    "                      \n",
    "                volume_1 = get_volume(sample, channel=channel_ind, frame=pair[0])\n",
    "                volume_2 = get_volume(sample, channel=channel_ind, frame=pair[1])\n",
    "                \n",
    "                if param.get('align'):\n",
    "                    volume_1, volume_2 = align_volumes(volume_1, volume_2)\n",
    "                    \n",
    "                if not is_test:  # We don't need to do this steps for test set\n",
    "                    if param.get('clean'):\n",
    "                        volume_1, volume_2 = clean_volumes(volume_1, volume_2)\n",
    "\n",
    "                    if param.get('augment'):\n",
    "                        volume_1 = augment_volume(volume_1) \n",
    "                        volume_2 = augment_volume(volume_2) \n",
    "                \n",
    "                if param.get('adap_hist'):\n",
    "                     volume_1 = adap_hist_volume(volume_1) \n",
    "                     volume_2 = adap_hist_volume(volume_2) \n",
    "                    \n",
    "                assert(len(volume_1) == len(volume_2))\n",
    "                print(f\"Channel: {channel_name}, Frames: {pair}, Number of samples: {len(volume_1)}\")\n",
    "                \n",
    "                train_or_test = \"val\" if is_test else \"train\"\n",
    "                path_1 = f\"{dataset_path}/{channel_name}/{train_or_test}/src/sample_{sample_ind}_pair{pair_ind}\"\n",
    "                path_2 = f\"{dataset_path}/{channel_name}/{train_or_test}/trg/sample_{sample_ind}_pair{pair_ind}\"\n",
    "                \n",
    "                save_volume(volume_1, path_1)\n",
    "                save_volume(volume_2, path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_dataset(dataset_folder):\n",
    "    if os.path.isdir(dataset_folder):\n",
    "        shutil.rmtree(dataset_folder, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found in total 9 files:\n",
      "18112019_SJR5_w1_30ms.nd2\n",
      "18112019_SJR5_w1_5ms.nd2\n",
      "18112019_SJR5_w2_30ms.nd2\n",
      "18112019_SJR5_w2_5ms.nd2\n",
      "18112019_SJR5_w3_30ms.nd2\n",
      "18112019_SJR5_w3_5ms.nd2\n",
      "18112019_SJR5_w4_30ms.nd2\n",
      "18112019_SJR5_w4_5ms.nd2\n",
      "18112019_SJR5_w5_30ms.nd2\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'data/'\n",
    "\n",
    "reader = nd2reader.ND2Reader\n",
    "samples, filenames = read_data(reader, DATA_PATH)\n",
    "print(f\"Found in total {len(filenames)} files:\")\n",
    "print(*filenames, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the dataset for experiment align\n",
      "Sample №0\n",
      "Channel: green, Frames: [0, 2], Number of samples: 35\n",
      "Channel: green, Frames: [1, 3], Number of samples: 35\n",
      "Channel: red, Frames: [0, 2], Number of samples: 35\n",
      "Channel: red, Frames: [1, 3], Number of samples: 35\n",
      "Sample №1\n",
      "Channel: green, Frames: [0, 2], Number of samples: 35\n",
      "Channel: green, Frames: [1, 3], Number of samples: 35\n",
      "Channel: red, Frames: [0, 2], Number of samples: 35\n",
      "Channel: red, Frames: [1, 3], Number of samples: 35\n",
      "Sample №2\n",
      "Channel: green, Frames: [0, 2], Number of samples: 35\n",
      "Channel: green, Frames: [1, 3], Number of samples: 35\n",
      "Channel: red, Frames: [0, 2], Number of samples: 35\n",
      "Channel: red, Frames: [1, 3], Number of samples: 35\n",
      "Sample №3\n",
      "Channel: green, Frames: [0, 2], Number of samples: 35\n",
      "Channel: green, Frames: [1, 3], Number of samples: 35\n",
      "Channel: red, Frames: [0, 2], Number of samples: 35\n",
      "Channel: red, Frames: [1, 3], Number of samples: 35\n",
      "Saving the dataset for experiment clean_align\n",
      "Sample №0\n",
      "Channel: green, Frames: [0, 2], Number of samples: 23\n",
      "Channel: green, Frames: [1, 3], Number of samples: 25\n",
      "Channel: red, Frames: [0, 2], Number of samples: 31\n",
      "Channel: red, Frames: [1, 3], Number of samples: 32\n",
      "Sample №1\n",
      "Channel: green, Frames: [0, 2], Number of samples: 12\n",
      "Channel: green, Frames: [1, 3], Number of samples: 12\n",
      "Channel: red, Frames: [0, 2], Number of samples: 30\n",
      "Channel: red, Frames: [1, 3], Number of samples: 31\n",
      "Sample №2\n",
      "Channel: green, Frames: [0, 2], Number of samples: 11\n",
      "Channel: green, Frames: [1, 3], Number of samples: 11\n",
      "Channel: red, Frames: [0, 2], Number of samples: 34\n",
      "Channel: red, Frames: [1, 3], Number of samples: 33\n",
      "Sample №3\n",
      "Channel: green, Frames: [0, 2], Number of samples: 17\n",
      "Channel: green, Frames: [1, 3], Number of samples: 35\n",
      "Channel: red, Frames: [0, 2], Number of samples: 32\n",
      "Channel: red, Frames: [1, 3], Number of samples: 35\n",
      "Saving the dataset for experiment align_augment\n",
      "Sample №0\n",
      "Channel: green, Frames: [0, 2], Number of samples: 140\n",
      "Channel: green, Frames: [1, 3], Number of samples: 140\n",
      "Channel: red, Frames: [0, 2], Number of samples: 140\n",
      "Channel: red, Frames: [1, 3], Number of samples: 140\n",
      "Sample №1\n",
      "Channel: green, Frames: [0, 2], Number of samples: 140\n",
      "Channel: green, Frames: [1, 3], Number of samples: 140\n",
      "Channel: red, Frames: [0, 2], Number of samples: 140\n",
      "Channel: red, Frames: [1, 3], Number of samples: 140\n",
      "Sample №2\n",
      "Channel: green, Frames: [0, 2], Number of samples: 140\n",
      "Channel: green, Frames: [1, 3], Number of samples: 140\n",
      "Channel: red, Frames: [0, 2], Number of samples: 140\n",
      "Channel: red, Frames: [1, 3], Number of samples: 140\n",
      "Sample №3\n",
      "Channel: green, Frames: [0, 2], Number of samples: 140\n",
      "Channel: green, Frames: [1, 3], Number of samples: 35\n",
      "Channel: red, Frames: [0, 2], Number of samples: 140\n",
      "Channel: red, Frames: [1, 3], Number of samples: 35\n",
      "Saving the dataset for experiment clean_align_augment\n",
      "Sample №0\n",
      "Channel: green, Frames: [0, 2], Number of samples: 92\n",
      "Channel: green, Frames: [1, 3], Number of samples: 100\n",
      "Channel: red, Frames: [0, 2], Number of samples: 124\n",
      "Channel: red, Frames: [1, 3], Number of samples: 128\n",
      "Sample №1\n",
      "Channel: green, Frames: [0, 2], Number of samples: 48\n",
      "Channel: green, Frames: [1, 3], Number of samples: 48\n",
      "Channel: red, Frames: [0, 2], Number of samples: 120\n",
      "Channel: red, Frames: [1, 3], Number of samples: 124\n",
      "Sample №2\n",
      "Channel: green, Frames: [0, 2], Number of samples: 44\n",
      "Channel: green, Frames: [1, 3], Number of samples: 44\n",
      "Channel: red, Frames: [0, 2], Number of samples: 136\n",
      "Channel: red, Frames: [1, 3], Number of samples: 132\n",
      "Sample №3\n",
      "Channel: green, Frames: [0, 2], Number of samples: 68\n",
      "Channel: green, Frames: [1, 3], Number of samples: 35\n",
      "Channel: red, Frames: [0, 2], Number of samples: 128\n",
      "Channel: red, Frames: [1, 3], Number of samples: 35\n"
     ]
    }
   ],
   "source": [
    "data_type = \"5ms\"\n",
    "file_indices = [ind for ind, filename in enumerate(filenames) if data_type in filename]\n",
    "interested_samples = [samples[ind] for ind in file_indices]\n",
    "    \n",
    "for experiment_ind, param in enumerate(params):\n",
    "    experiment_name = \"_\".join([key for key, el in param.items() if el])\n",
    "        \n",
    "    print(f\"Generating the dataset for experiment {experiment_name}\")\n",
    "    dataset_path = f\"{experiment_name}_data_{data_type}\"\n",
    "    \n",
    "    delete_dataset(dataset_path)\n",
    "    generate_dataset(interested_samples, param, dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [{'clean':False, 'align':True, 'augment':False, 'adap_hist': True}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the dataset for experiment align_adap_hist\n",
      "Sample №0\n",
      "Channel: green, Frames: [0, 2], Number of samples: 35\n",
      "Channel: green, Frames: [1, 3], Number of samples: 35\n",
      "Channel: red, Frames: [0, 2], Number of samples: 35\n",
      "Channel: red, Frames: [1, 3], Number of samples: 35\n",
      "Sample №1\n",
      "Channel: green, Frames: [0, 2], Number of samples: 35\n",
      "Channel: green, Frames: [1, 3], Number of samples: 35\n",
      "Channel: red, Frames: [0, 2], Number of samples: 35\n",
      "Channel: red, Frames: [1, 3], Number of samples: 35\n",
      "Sample №2\n",
      "Channel: green, Frames: [0, 2], Number of samples: 35\n",
      "Channel: green, Frames: [1, 3], Number of samples: 35\n",
      "Channel: red, Frames: [0, 2], Number of samples: 35\n",
      "Channel: red, Frames: [1, 3], Number of samples: 35\n",
      "Sample №3\n",
      "Channel: green, Frames: [0, 2], Number of samples: 35\n",
      "Channel: green, Frames: [1, 3], Number of samples: 35\n",
      "Channel: red, Frames: [0, 2], Number of samples: 35\n",
      "Channel: red, Frames: [1, 3], Number of samples: 35\n"
     ]
    }
   ],
   "source": [
    "data_type = \"5ms\"\n",
    "file_indices = [ind for ind, filename in enumerate(filenames) if data_type in filename]\n",
    "interested_samples = [samples[ind] for ind in file_indices]\n",
    "    \n",
    "for experiment_ind, param in enumerate(params):\n",
    "    experiment_name = \"_\".join([key for key, el in param.items() if el])\n",
    "        \n",
    "    print(f\"Generating the dataset for experiment {experiment_name}\")\n",
    "    dataset_path = f\"{experiment_name}_data_{data_type}\"\n",
    "    \n",
    "    delete_dataset(dataset_path)\n",
    "    generate_dataset(interested_samples, param, dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
